# local-llm-mcp-integration
Local LLM deployment using Docker, Ollama and a custom MCP server with REST-based API integration.
